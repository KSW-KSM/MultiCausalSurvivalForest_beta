# Data Simulation
p <- 20
# Data Simulation
xx_all <- matrix(runif(num*p), nrow = num, ncol=p)
# lmdh controls what signal is being used.
sigma=lmdh*(xx_all[,5]>0.5)+0.6*(sqrt(abs(xx_all[,5]*xx_all[,7])))
# Creating the failure, censoring and observed time.
# time <- rexp(n=num,rate=1/sigma)
time <- exp(sigma + rnorm(num,0,1))
return(list(time,xx_all,sigma))
}
M <-1000
colmat <- matrix(0,ncol=3,nrow=M)
for(j in 1:M){
dat.list <-gen.data.test(j,10000,1)
colmat[j,] = quantile(dat.list[[1]],probs=c(0.25,0.5,0.75))
}
colMeans(colmat)
x <- 1
assert_that(length(x) == 3)
library(rmarkdown)
file.edit("path/to/file.Rmd")
getwd()
file.edit("C:/Users/yvc5154/Documents/file.Rmd")
rmarkdown::pandoc_version()
rmarkdown::pandoc_available()
18000*4+24000
library(partykit)
?ctree
# random forest
### Clean Memory ###
rm(list=ls())
#args <- commandArgs(trailingOnly = TRUE)
#aa <- args[1]
#print(aa)
#print(args[1])
#print(as.integer(args[1]))
options(warn = -1)
### Call Libraries ###
library(rpart)
library(survival)
library(prodlim)
library(timereg)
#library(etm)
library(randomForestSRC)
library(MASS)
library(pec)
ptm <- proc.time()
setwd("C:/CRgrf")
source("sourceCpp.R")
rho = 0.9
nump = 20
Sig <- matrix(0,ncol=nump,nrow=nump)
for(r in 1:nump){
for(v in 1:nump){
if(r==v){
Sig[r,v] <- 1
}else{
Sig[r,v] <- rho^(abs(r-v))
}
}
}
### Generate training data
gen.data.tr <- function(myseed,n,p0,beta10_z2,beta20_z2,beta10_z1,beta20_z1,beta10_z0,beta20_z0) {
set.seed(myseed)
X <- mvrnorm(n,mu = rep(0,20),Sigma = Sig)
x1 <- X[,1];x2 <- X[,2];x3 <- X[,3]
x4 <- X[,4];x5 <- X[,5];x6 <- X[,6]
x7 <- X[,7];x8 <- X[,8];x9 <- X[,9]
x10 <- X[,10];x11 <- X[,11];x12 <- X[,12]
x13 <- X[,13];x14 <- X[,14];x15 <- X[,15]
x16 <- X[,16];x17 <- X[,17];x18 <- X[,18]
x19 <- X[,19];x20 <- X[,20]
xx.true <- cbind(1,sin(pi*x1*x2),sqrt(abs(x3)),x10,as.numeric(x11 > 0),abs(x15))
index.temp_z2 <- rbinom(n,1,1-(1-p0)^exp(xx.true %*% beta10_z2))
index_z2 <- 2 - index.temp_z2
index.temp_z1 <- rbinom(n,1,1-(1-p0)^exp(xx.true %*% beta10_z1))
index_z1 <- 2 - index.temp_z1
index.temp_z0 <- rbinom(n,1,1-(1-p0)^exp(xx.true %*% beta10_z0))
index_z0 <- 2 - index.temp_z0
event.comp.time.z2 <- rep(0,n)
event.comp.time.z1 <- rep(0,n)
event.comp.time.z0 <- rep(0,n)
for(i in 1:n){
if(index_z2[i] == 1){
u <- runif(1,0,1)
event.comp.time.z2[i] <- -log(1-(1-(1-u*(1-(1-p0)^exp(t(xx.true[i,]) %*% beta10_z2)))^(1/exp(t(xx.true[i,]) %*% beta10_z2)))/p0)
}else{
#v <- runif(1,0,1)
event.comp.time.z2[i] <- rexp(1,exp(t(xx.true[i,]) %*% beta20_z2))
}
}
for(i in 1:n){
if(index_z1[i] == 1){
u <- runif(1,0,1)
event.comp.time.z1[i] <- -log(1-(1-(1-u*(1-(1-p0)^exp(t(xx.true[i,]) %*% beta10_z1)))^(1/exp(t(xx.true[i,]) %*% beta10_z1)))/p0)
}else{
#v <- runif(1,0,1)
event.comp.time.z1[i] <- rexp(1,exp(t(xx.true[i,]) %*% beta20_z1))
}
}
for(i in 1:n){
if(index_z0[i] == 1){
u <- runif(1,0,1)
event.comp.time.z0[i] <- -log(1-(1-(1-u*(1-(1-p0)^exp(t(xx.true[i,]) %*% beta10_z0)))^(1/exp(t(xx.true[i,]) %*% beta10_z0)))/p0)
}else{
#v <- runif(1,0,1)
event.comp.time.z0[i] <- rexp(1,exp(t(xx.true[i,]) %*% beta20_z0))
}
}
alpha01 <- 1
alpha11 <- 0.5
alpha21 <- -0.3
alpha02 <- 0.7
alpha12 <- 0.3
alpha22 <- -0.5
logit.lp1 <- alpha01 + x1*alpha11 + x2*alpha21
logit.lp2 <- alpha02 + x1*alpha12 + x2*alpha22
probs_z1 <- exp(logit.lp1)/(1+exp(logit.lp1 + logit.lp2))
probs_z2 <- exp(logit.lp2)/(1+exp(logit.lp1 + logit.lp2))
z <- sample(0:2, replace=T)
event.comp.time <- as.numeric(z==1)*event.comp.time.z1 + (1-z)*event.comp.time.z0
index <- z*index_z1 + (1-z)*index_z0
#cen <- rexp(n,1.475) #beta10 = 3
#cen <- rexp(n,0.75) #beta10 = 2
mu <- x1+x3+x5
cen <- exp(rnorm(n,mu+0.1,1))
#cen <- runif(n,0,2.83) #beta10 = 2
#cen <- runif(n,0,3.275) #beta10 = 1
#cen <- runif(n,0,3.075) #beta10 = 1.5
return(list(event.comp.time,index,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,cen,z))
}
# Length of data vector created
num = 2500
# Length of dataset used to fit model, num - nu is length of test set.
nu=500
test_len <- num - nu
preset=(nu+1):num
nsim <- 5
beta10_true_z1=c(-1,rep(0.5,3),0.6,-0.3)
beta20_true_z1=c(-1.5,rep(-0.5,3),0.5,0.1)
beta10_true_z0=c(rep(0.3,4),0.8,-0.1)
beta20_true_z0=c(0,rep(-0.3,3),0.3,0.2)
p0 = 0.5
# Find time points
km_times <- c(0.3559963, 1.0384856, 2.9275240)
# set seed - possible to change any numbers
seednum <- 1  #as.integer(args[1])
seednum
data.tr.temp <- gen.data.tr(seednum, nu, p0=0.5, beta10 = beta10_true, beta20 = beta20_true)
### Generate training data
gen.data.tr <- function(myseed,n,p0,beta10,beta20) {
set.seed(myseed)
X <- mvrnorm(n,mu = rep(0,20),Sigma = Sig)
x1 <- X[,1];x2 <- X[,2];x3 <- X[,3]
x4 <- X[,4];x5 <- X[,5];x6 <- X[,6]
x7 <- X[,7];x8 <- X[,8];x9 <- X[,9]
x10 <- X[,10];x11 <- X[,11];x12 <- X[,12]
x13 <- X[,13];x14 <- X[,14];x15 <- X[,15]
x16 <- X[,16];x17 <- X[,17];x18 <- X[,18]
x19 <- X[,19];x20 <- X[,20]
z <- sample(0:2, size = n, replace=T)
z1 <- as.numeric(z==1);z2 <- as.numeric(z==2)
xx.true <- cbind(z1,z2,sin(pi*x1*x2),sqrt(abs(x3)),x10,as.numeric(x11 > 0),abs(x15))
index.temp <- rbinom(n,1,1-(1-p0)^exp(xx.true %*% beta10))
index <- 2 - index.temp
event.comp.time <- rep(0,n)
for(i in 1:n){
if(index[i] == 1){
u <- runif(1,0,1)
event.comp.time[i] <- -log(1-(1-(1-u*(1-(1-p0)^exp(t(xx.true[i,]) %*% beta10)))^(1/exp(t(xx.true[i,]) %*% beta10)))/p0)
}else{
#v <- runif(1,0,1)
event.comp.time[i] <- rexp(1,exp(t(xx.true[i,]) %*% beta20))
}
}
# alpha0 <- 1
#  alpha1 <- 0.5
#  alpha2 <- -0.3
#  logit.lp <- alpha0 + x1*alpha1 + x2*alpha2
#  probs_z <- exp(logit.lp)/(1+exp(logit.lp))
#  z <- rbinom(n,1,probs_z)
#  event.comp.time <- z*event.comp.time.z1 + (1-z)*event.comp.time.z0
#  index <- z*index_z1 + (1-z)*index_z0
#cen <- rexp(n,1.475) #beta10 = 3
#cen <- rexp(n,0.75) #beta10 = 2
mu <- x1+x3+x5
cen <- exp(rnorm(n,mu+0.1,1))
#cen <- runif(n,0,2.83) #beta10 = 2
#cen <- runif(n,0,3.275) #beta10 = 1
#cen <- runif(n,0,3.075) #beta10 = 1.5
return(list(event.comp.time,index,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,cen,z,z1,z2))
}
# Length of data vector created
num = 2500
# Length of dataset used to fit model, num - nu is length of test set.
nu=500
test_len <- num - nu
preset=(nu+1):num
nsim <- 5
beta10_true_z1=c(-1,rep(0.5,3),0.6,-0.3)
beta20_true_z1=c(-1.5,rep(-0.5,3),0.5,0.1)
beta10_true_z0=c(rep(0.3,4),0.8,-0.1)
beta20_true_z0=c(0,rep(-0.3,3),0.3,0.2)
p0 = 0.5
# Find time points
km_times <- c(0.3559963, 1.0384856, 2.9275240)
# set seed - possible to change any numbers
seednum <- 1  #as.integer(args[1])
seednum
data.tr.temp <- gen.data.tr(seednum, nu, p0=0.5, beta10 = beta10_true, beta20 = beta20_true)
beta10_true=c(-1,-0.7,rep(0.5,3),0.6,-0.3)
beta20_true=c(-1.5,-0.4,rep(-0.5,3),0.5,0.1)
p0 = 0.5
# Find time points
km_times <- c(0.3559963, 1.0384856, 2.9275240)
# set seed - possible to change any numbers
seednum <- 1  #as.integer(args[1])
seednum
data.tr.temp <- gen.data.tr(seednum, nu, p0=0.5, beta10 = beta10_true, beta20 = beta20_true)
data.test.temp <- gen.data.test(seednum, test_len, p0=0.5, beta10_z1 = beta10_true_z1, beta20_z1 = beta20_true_z1, beta10_z0 = beta10_true_z0, beta20_z0 = beta20_true_z0)
data.tr.temp <- gen.data.tr(seednum, nu, p0=0.5, beta10 = beta10_true, beta20 = beta20_true)
event.time.temp <- data.tr.temp[[1]]
cen.temp <- data.tr.temp[[23]]
z.temp <- data.tr.temp[[24]]
z1.temp <- data.tr.temp[[25]]
z2.temp <- data.tr.temp[[26]]
obs.time.temp <- pmin(event.time.temp, cen.temp)
status.woc.temp <- data.tr.temp[[2]]
status.temp <- as.numeric(event.time.temp <= cen.temp) * status.woc.temp
status.all.temp <- as.numeric(event.time.temp <= cen.temp)
x1.temp <- data.tr.temp[[3]];x2.temp <- data.tr.temp[[4]];x3.temp <- data.tr.temp[[5]]
x4.temp <- data.tr.temp[[6]];x5.temp <- data.tr.temp[[7]];x6.temp <- data.tr.temp[[8]]
x7.temp <- data.tr.temp[[9]];x8.temp <- data.tr.temp[[10]];x9.temp <- data.tr.temp[[11]]
x10.temp <- data.tr.temp[[12]];x11.temp <- data.tr.temp[[13]];x12.temp <- data.tr.temp[[14]]
x13.temp <- data.tr.temp[[15]];x14.temp <- data.tr.temp[[16]];x15.temp <- data.tr.temp[[17]]
x16.temp <- data.tr.temp[[18]];x17.temp <- data.tr.temp[[19]];x18.temp <- data.tr.temp[[20]]
x19.temp <- data.tr.temp[[21]];x20.temp <- data.tr.temp[[22]]
# Creating the dataset
dat.tr <- data.frame(obs=obs.time.temp,status = status.temp,status.all=status.all.temp,z = z.temp, z1 = z1.temp, z2 = z2.temp, x1=x1.temp, x2=x2.temp, x3=x3.temp, x4=x4.temp, x5=x5.temp, x6=x6.temp, x7=x7.temp, x8=x8.temp, x9=x9.temp, x10=x10.temp,x11=x11.temp, x12=x12.temp, x13=x13.temp, x14=x14.temp, x15=x15.temp, x16=x16.temp, x17=x17.temp, x18=x18.temp, x19=x19.temp, x20=x20.temp)
source("parameter_cr.R")
source("parameters_cr.R")
p <- 20
source("parameters_cr.R")
dat.tr$status.all
horizon = km_times[1]
if (is.null(horizon) || !is.numeric(horizon) || length(horizon) != 1) {
stop("The `horizon` argument defining the estimand is required.")
}
X <- dat.tr[,7:(p+6)]
W <- dat.tr$z
#W <- sample(0:2, size=nu, replace=T)
#dat.tr$z <- W
dat.tr$z1 <- as.numeric(W==1)
dat.tr$z2 <- as.numeric(W==2)
Y <- dat.tr$obs
D <- dat.tr$status.all
e <- dat.tr$status
has.missing.values <- validate_X(X, allow.na = TRUE)
validate_sample_weights(sample.weights, X)
Y <- validate_observations(Y, X)
W <- validate_observations(W, X)
D <- validate_observations(D, X)
clusters <- validate_clusters(clusters, X)
samples.per.cluster <- validate_equalize_cluster_weights(equalize.cluster.weights, clusters, sample.weights)
num.threads <- validate_num_threads(num.threads)
if (any(Y < 0)) {
stop("The event times must be non-negative.")
}
#  if (!all(W %in% c(0, 1))) {
#   stop("The treatment values can only be 0 or 1.")
#  }
if (!all(D %in% c(0, 1))) {
stop("The censor values can only be 0 or 1.")
}
if (sum(D) == 0) {
stop("All observations are censored.")
}
fY <- as.numeric(Y <= horizon & e == 1)
if (is.null(failure.times)) {
Y.grid <- sort(unique(Y))
} else if (min(Y) < min(failure.times)) {
stop("If provided, `failure.times` should be a grid starting on or before min(Y).")
} else {
Y.grid <- failure.times
}
if (length(Y.grid) <= 2) {
stop("The number of distinct event times should be more than 2.")
}
if (horizon < min(Y.grid)) {
stop("`horizon` cannot be before the first event.")
}
if (nrow(X) > 5000 && length(Y.grid) / nrow(X) > 0.1) {
warning(paste0("The number of events are more than 10% of the sample size. ",
"To reduce the computational burden of fitting survival and ",
"censoring curves, consider discretizing the event values `Y` or ",
"supplying a coarser grid with the `failure.times` argument. "), immediate. = TRUE)
}
#if (is.null(W.hat)) {
#  forest.W <- regression_forest(X, W, num.trees = max(50, num.trees / 4),
#                               sample.weights = sample.weights, clusters = clusters,
#                              equalize.cluster.weights = equalize.cluster.weights,
#                             sample.fraction = sample.fraction, mtry = mtry,
#                            min.node.size = 5, honesty = TRUE,
#                           honesty.fraction = 0.5, honesty.prune.leaves = TRUE,
#                          alpha = alpha, imbalance.penalty = imbalance.penalty,
#                         ci.group.size = 1, tune.parameters = tune.parameters,
#                        compute.oob.predictions = TRUE,
#                        num.threads = num.threads, seed = seed, mahalanobis)
#    W.hat <- predict(forest.W)$predictions
#  } else if (length(W.hat) == 1) {
#    W.hat <- rep(W.hat, nrow(X))
#  } else if (length(W.hat) != nrow(X)) {
#    stop("W.hat has incorrect length.")
#  }
# Compute (generalized) propensity score
if (is.null(W.hat) & length(unique(W)) > 2) {
forest.W <- probability_forest(X, factor(W), num.trees = max(50, num.trees / 4),
sample.weights = sample.weights, clusters = clusters,
equalize.cluster.weights = equalize.cluster.weights,
sample.fraction = sample.fraction, mtry = mtry,
min.node.size = 5, honesty = TRUE,
honesty.fraction = 0.5, honesty.prune.leaves = TRUE,
alpha = alpha, imbalance.penalty = imbalance.penalty,
ci.group.size = 1,
compute.oob.predictions = TRUE,
num.threads = num.threads, seed = seed)
W.hat <- predict(forest.W)$predictions
}
library(grf)
# Compute (generalized) propensity score
if (is.null(W.hat) & length(unique(W)) > 2) {
forest.W <- probability_forest(X, factor(W), num.trees = max(50, num.trees / 4),
sample.weights = sample.weights, clusters = clusters,
equalize.cluster.weights = equalize.cluster.weights,
sample.fraction = sample.fraction, mtry = mtry,
min.node.size = 5, honesty = TRUE,
honesty.fraction = 0.5, honesty.prune.leaves = TRUE,
alpha = alpha, imbalance.penalty = imbalance.penalty,
ci.group.size = 1,
compute.oob.predictions = TRUE,
num.threads = num.threads, seed = seed)
W.hat <- predict(forest.W)$predictions
}
W.centered1 <- as.numeric(W==1) - W.hat[,2]
W.centered2 <- as.numeric(W==2) - W.hat[,3]
args.nuisance_surv <- list(failure.times = failure.times,
num.trees = max(50, min(num.trees / 4, 500)),
sample.weights = sample.weights,
clusters = clusters,
equalize.cluster.weights = equalize.cluster.weights,
sample.fraction = sample.fraction,
mtry = mtry,
min.node.size = 15,
honesty = TRUE,
honesty.fraction = 0.5,
honesty.prune.leaves = TRUE,
alpha = alpha,
prediction.type = "Kaplan-Meier", # to guarantee non-zero estimates.
compute.oob.predictions = FALSE,
num.threads = num.threads,
seed = seed)
#dat.tr$z1 <- as.numeric(dat.tr$z==1)
#dat.tr$z2 <- as.numeric(dat.tr$z==2)
# E[f(T) | X] = e_1(X) E[f(T) | X, Z = 1] + e_2(X) E[f(T) | X, Z = 2] + (1 - e_1(X)-e_2(X)) E[f(T) | X, Z = 0]
form<-"Surv(obs,status)~z1+z2+x1"
for (count in 2:p){
form<-paste(form,"+x",as.character(count),sep="")
}
fit_forest <- rfsrc(as.formula(form), data=dat.tr, nodesize=20, samptype = "swr")
# prediction
dat.sw.tr0 <- dat.tr
dat.sw.tr1 <- dat.tr
dat.sw.tr2 <- dat.tr
# for Z=0
dat.sw.tr0$z1 <- 0
dat.sw.tr0$z2 <- 0
# for Z=1
dat.sw.tr1$z1 <- 1
dat.sw.tr1$z2 <- 0
# for Z=2
dat.sw.tr2$z1 <- 0
dat.sw.tr2$z2 <- 1
pred.fit.o <- predict(fit_forest)
pred.fit.z0 <- predict(fit_forest,newdata = dat.sw.tr0)
pred.fit.z1 <- predict(fit_forest,newdata = dat.sw.tr1)
pred.fit.z2 <- predict(fit_forest,newdata = dat.sw.tr2)
n <- nu
pred.mat.e1.z2 <- matrix(0, n, n)
pred.mat.e1.z1 <- matrix(0, n, n)
pred.mat.e1.z0 <- matrix(0, n, n)
pred.mat.e1.o <- matrix(0, n, n)
pred.mat.e2.o <- matrix(0, n, n)
a <- round(sort(dat.tr$obs),8)
b <- round(fit_forest$time.interest,8)
#intersect(a,b)
ind <- which(a %in% b)
pred.mat.e1.z0[,ind] <- pred.fit.z0$cif[,,1]
pred.mat.e1.z1[,ind] <- pred.fit.z1$cif[,,1]
pred.mat.e1.z2[,ind] <- pred.fit.z2$cif[,,1]
pred.mat.e1.o[,ind] <- pred.fit.o$cif.oob[,,1]
pred.mat.e2.o[,ind] <- pred.fit.o$cif.oob[,,2]
tlen <- 1:n
clen <- tlen[-ind]
if(clen[1]==1){
pred.mat.e1.o[,clen[1]] <- 0
pred.mat.e2.o[,clen[1]] <- 0
pred.mat.e1.z2[,clen[1]] <- 0
pred.mat.e1.z1[,clen[1]] <- 0
pred.mat.e1.z0[,clen[1]] <- 0
for(j in 2:length(clen)){
pred.mat.e1.o[,clen[j]] <- pred.mat.e1.o[,(clen[j] - 1)]
pred.mat.e2.o[,clen[j]] <- pred.mat.e2.o[,(clen[j] - 1)]
pred.mat.e1.z2[,clen[j]] <- pred.mat.e1.z2[,(clen[j] - 1)]
pred.mat.e1.z1[,clen[j]] <- pred.mat.e1.z1[,(clen[j] - 1)]
pred.mat.e1.z0[,clen[j]] <- pred.mat.e1.z0[,(clen[j] - 1)]
}
}else{
for(j in 1:length(clen)){
pred.mat.e1.o[,clen[j]] <- pred.mat.e1.o[,(clen[j] - 1)]
pred.mat.e2.o[,clen[j]] <- pred.mat.e2.o[,(clen[j] - 1)]
pred.mat.e1.z2[,clen[j]] <- pred.mat.e1.z2[,(clen[j] - 1)]
pred.mat.e1.z1[,clen[j]] <- pred.mat.e1.z1[,(clen[j] - 1)]
pred.mat.e1.z0[,clen[j]] <- pred.mat.e1.z0[,(clen[j] - 1)]
}
}
S.hat <- 1 - pred.mat.e1.o - pred.mat.e2.o
pred.mat.e1.z2.f <- pred.mat.e1.z2[,ind]
pred.mat.e1.z1.f <- pred.mat.e1.z1[,ind]
pred.mat.e1.z0.f <- pred.mat.e1.z0[,ind]
horizonS.index <- findInterval(horizon, fit_forest$time.interest)
if (horizonS.index == 0) {
Y.hat <- rep(0, nrow(X))
} else {
#Y.hat <- W.hat * pred.mat.e1.z1.f[, horizonS.index] + (1 - W.hat) * pred.mat.e1.z0.f[, horizonS.index]
Y.hat <- W.hat[,2] * pred.mat.e1.z1.f[, horizonS.index] + W.hat[,3] * pred.mat.e1.z2.f[, horizonS.index] +
(1 - W.hat[,2]-W.hat[,3]) * pred.mat.e1.z0.f[, horizonS.index]
}
# The conditional survival function for the censoring process S_C(t, x, w).
args.nuisance_surv$compute.oob.predictions <- TRUE
sf.censor <- do.call(survival_forest, c(list(X = cbind(X, dat.tr$z1, dat.tr$z2), Y = Y, D = 1 - D), args.nuisance_surv))
predict(sf.censor, failure.times = Y.grid)$predictions$s1
C.hat <- predict(sf.censor, failure.times = Y.grid)$predictions$s1
# if (target == "survival.probability") {
# Evaluate psi up to horizon
D[Y > horizon] <- 1
Y[Y > horizon] <- horizon
# }
Y.index <- findInterval(Y, Y.grid) # (invariance: Y.index > 0)
C.Y.hat <- C.hat[cbind(seq_along(Y.index), Y.index)] # Pick out P[Ci > Yi | Xi, Wi]
# if (target == "RMST" && any(C.Y.hat <= 0.05)) {
#   warning(paste("Estimated censoring probabilities go as low as:", round(min(C.Y.hat), 5),
#                 "- an identifying assumption is that there exists a fixed positive constant M",
#                 "such that the probability of observing an event past the maximum follow-up time ",
#                 "is at least M (i.e. P(T > horizon | X) > M).",
#                 "This warning appears when M is less than 0.05, at which point causal survival forest",
#                 "can not be expected to deliver reliable estimates."), immediate. = TRUE)
# } else if (target == "RMST" && any(C.Y.hat < 0.2)) {
#   warning(paste("Estimated censoring probabilities are lower than 0.2",
#                 "- an identifying assumption is that there exists a fixed positive constant M",
#                 "such that the probability of observing an event past the maximum follow-up time ",
#                 "is at least M (i.e. P(T > horizon | X) > M)."))
# } else if (target == "survival.probability" && any(C.Y.hat <= 0.001)) {
#   warning(paste("Estimated censoring probabilities go as low as:", round(min(C.Y.hat), 5),
#                 "- forest estimates will likely be very unstable, a larger target `horizon`",
#                 "is recommended."), immediate. = TRUE)
# } else if (target == "survival.probability" && any(C.Y.hat < 0.05)) {
#  warning(paste("Estimated censoring probabilities are lower than 0.05",
#                "and forest estimates may not be stable. Using a smaller target `horizon`",
#                "may help."))
#}
#  eta <- compute_eta(S.hat, C.hat, C.Y.hat, Y.hat, W.centered,
#                     D, fY, Y.index, Y.grid, target, horizon)
horizon.cr.index <- findInterval(horizon, Y.grid)
# Compute P(t < T <= horizon, e=1)
Q.num.hat <- matrix(0,n,n)
Q.hat <- matrix(0,n,n) # Q(t, X) =  P(T_i <= horizon, e=1 | Z_i, T_i > t)
for(j in 1:n){
Q.num.hat[,j] <- pred.mat.e1.o[, horizon.cr.index]-pred.mat.e1.o[,j]
Q.hat[,j] <- Q.num.hat[,j]/S.hat[,j]
}
Q.hat[, horizon.cr.index:ncol(Q.hat)] <- 0
# Pick out Q(Yi, X)
Q.Y.hat <- Q.hat[cbind(seq_along(Y.index), Y.index)]
numerator.one1 <- (D * (fY - Y.hat) + (1 - D) * (Q.Y.hat - Y.hat)) * W.centered1 / C.Y.hat
numerator.one2 <- (D * (fY - Y.hat) + (1 - D) * (Q.Y.hat - Y.hat)) * W.centered2 / C.Y.hat
# The conditional hazard function differential -d log(C.hat(t, x, w))
# This simple forward difference approximation works reasonably well.
# (note the "/dt" term is not needed as it cancels out in the lambda.C.hat / C.hat integral)
log.surv.C <- -log(cbind(1, C.hat))
dlambda.C.hat <- log.surv.C[, 2:(ncol(C.hat) + 1)] - log.surv.C[, 1:ncol(C.hat)]
integrand <- dlambda.C.hat / C.hat * (Q.hat - Y.hat)
numerator.two1 <- rep(0, length(Y.index))
numerator.two2 <- rep(0, length(Y.index))
for (sample in seq_along(Y.index)) {
Yi.index <- Y.index[sample]
numerator.two1[sample] <- sum(integrand[sample, seq_len(Yi.index)]) * W.centered1[sample]
numerator.two2[sample] <- sum(integrand[sample, seq_len(Yi.index)]) * W.centered2[sample]
}
### doubly robust ###
numerator1 <- numerator.one1 - numerator.two1
denominator1 <- W.centered1^2
numerator2 <- numerator.one2 - numerator.two2
denominator2 <- W.centered2^2
# denominator simplifies to this.
eta <- list(numerator1 = numerator1, denominator1 = denominator1,numerator2 = numerator2, denominator2 = denominator2,
numerator.one1 = numerator.one1, numerator.two1 = numerator.two1, numerator.one2 = numerator.one2, numerator.two2 = numerator.two2,
C.Y.hat = C.Y.hat)
validate_observations(eta[["numerator1"]], X)
validate_observations(eta[["denominator1"]], X)
validate_observations(eta[["numerator2"]], X)
validate_observations(eta[["denominator2"]], X)
